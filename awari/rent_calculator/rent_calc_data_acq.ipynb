{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Data Acquisition"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_URL = 'https://www.vivareal.com.br/aluguel/sp/sao-paulo/apartamento_residencial/?pagina='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get ads in multiple pages and returns a list of dataFrames\n",
    "def scrap(url, pages=2):\n",
    "    '''Scraps search results from a real state website'''\n",
    "    if pages < 2:\n",
    "        raise ValueError('number of pages must be greater than 2')\n",
    "    scrap_df = [] # list to hold all data frames created per page\n",
    "    for p_no in range(1, pages):\n",
    "        search_url = url + str(p_no)\n",
    "        # check if request is successful\n",
    "        try:\n",
    "            page_content = requests.get(search_url).content\n",
    "            print(search_url)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to gather data from {search_url}')\n",
    "            print(e)\n",
    "    \n",
    "        soup = BeautifulSoup(page_content) # get page content into soup\n",
    "\n",
    "        rent_tag_list = soup.find_all('div', {'class': 'property-card__price js-property-card-prices js-property-card__price-small'}) # gets all rent tags\n",
    "        rent_text_list = [rent.p.text for rent in rent_tag_list] # gets the text from each rent tag\n",
    "\n",
    "\n",
    "        rooms_tag_list = soup.find_all('li', {'class': 'property-card__detail-item property-card__detail-room js-property-detail-rooms'})\n",
    "        rooms_text_list = [room.span.text for room in rooms_tag_list]\n",
    "\n",
    "\n",
    "        address_tag_list = soup.find_all('span', {'class': 'property-card__address'})\n",
    "        address_text_list = [address.text for address in address_tag_list]\n",
    "\n",
    "\n",
    "        bathroom_tag_list = soup.find_all('li', {'class': 'property-card__detail-item property-card__detail-bathroom js-property-detail-bathroom'})\n",
    "        bathroom_text_list = [bathroom.span.text for bathroom in bathroom_tag_list]\n",
    "\n",
    "\n",
    "        parking_tag_list = soup.find_all('li', {'class': 'property-card__detail-item property-card__detail-garage js-property-detail-garages'})\n",
    "        parking_text_list = [parking.span.text for parking in parking_tag_list]\n",
    "\n",
    "        area_tag_list = soup.find_all('span', {'class': 'property-card__detail-value js-property-card-value property-card__detail-area js-property-card-detail-area'})\n",
    "        area_text_list = [area.text for area in area_tag_list]\n",
    "\n",
    "        # create a dictionary to create a pandas dataframe\n",
    "        data = {'address': address_text_list,\n",
    "                'rent': rent_text_list,\n",
    "                'rooms': rooms_text_list,\n",
    "                'bathroom': bathroom_text_list,\n",
    "                'parking': parking_text_list,\n",
    "                'area': area_text_list}\n",
    "        \n",
    "        #create the dataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        print(df)\n",
    "        scrap_df.append(df)\n",
    "    \n",
    "    final_data = pd.concat(scrap_df)\n",
    "    # return final dataFrame\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "final_data = scrap(base_URL, 180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head(72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data['suburb'] = final_data.address.str.split('\\s-').str[1].str.split(', ').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.nunique(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.drop_duplicates(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}